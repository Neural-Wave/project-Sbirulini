{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from PIL import Image, ImageOps\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from base import BaseModel\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from torchvision.transforms import v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load labels from the JSON file\n",
    "json_file_path = '/teamspace/studios/this_studio/data/final_labels_good_light_4551.json'  # Replace with your JSON file path\n",
    "\n",
    "with open(json_file_path, 'r') as f:\n",
    "    data_dict = json.load(f)\n",
    "\n",
    "# Map labels to 'aligned' or 'not_aligned'\n",
    "mapped_labels = {}\n",
    "for img_name, label in data_dict.items():\n",
    "    if label == 'aligned':\n",
    "        mapped_labels[img_name] = 'aligned'\n",
    "    else:\n",
    "        mapped_labels[img_name] = 'not_aligned'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load labels from the JSON file\n",
    "json_file_path = '/teamspace/studios/this_studio/data/label_bad_light.json'  # Replace with your JSON file path\n",
    "\n",
    "with open(json_file_path, 'r') as f:\n",
    "    data_dict = json.load(f)\n",
    "\n",
    "# Map labels to 'aligned' or 'not_aligned'\n",
    "mapped_labels2 = {}\n",
    "for img_name, label in data_dict.items():\n",
    "    if label == 'aligned':\n",
    "        mapped_labels2[img_name] = 'aligned'\n",
    "    else:\n",
    "        mapped_labels2[img_name] = 'not_aligned'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, annotations, img_dir, transform=None):\n",
    "        self.img_labels = []\n",
    "        for img_name, label in annotations.items():\n",
    "            # Map label to numerical value\n",
    "            if label == 'aligned':\n",
    "                mapped_label = torch.tensor([1.])  # Class 0\n",
    "            else:\n",
    "                mapped_label = torch.tensor([0.])  # Class 1 ('not_aligned')\n",
    "            self.img_labels.append((img_name, mapped_label))\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name, label = self.img_labels[idx]\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        # Open the image file\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrDataset(Dataset):\n",
    "  def __init__(self, base_dataset, transformations):\n",
    "    super(TrDataset, self).__init__()\n",
    "    self.base = base_dataset\n",
    "    self.transformations = transformations\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.base)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    x, y = self.base[idx]\n",
    "    return self.transformations(x), y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_in_half(img):\n",
    "    width, height = img.size\n",
    "    # print(width, height)\n",
    "    intermediate = img.crop((width // 2, 0, width, height))\n",
    "    width, height = intermediate.size\n",
    "    # print(width, height)\n",
    "    return ImageOps.equalize(intermediate.resize((width//2, height//2), Image.Resampling.LANCZOS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    # transforms.Resize((224, 224)),  # Resize images to 224x224 pixels\n",
    "    cut_in_half,\n",
    "    transforms.ToTensor(),  # Convert PIL Image to tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # Normalize to ImageNet standards\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "    # v2.RandomPerspective(distortion_scale=0.6, p=1.0),\n",
    "    # v2.RandomAffine(degrees=(30, 70), translate=(0.1, 0.3), scale=(0.5, 0.75)),\n",
    "    # v2.ElasticTransform(alpha=250.0),\n",
    "    # v2.RandomRotation(degrees=(0, 90))\n",
    "])\n",
    "transform2 = transforms.Compose([\n",
    "    # transforms.Resize((224, 224)),  # Resize images to 224x224 pixels\n",
    "    cut_in_half,\n",
    "    transforms.ToTensor(),  # Convert PIL Image to tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # Normalize to ImageNet standards\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the directory where your images are stored\n",
    "\n",
    "img_dir = '/teamspace/studios/this_studio/data/train_set/good_light'  # Replace with your image directory\n",
    "img_dir2 = '/teamspace/studios/this_studio/data/train_set/bad_light'\n",
    "\n",
    "# Create the dataset\n",
    "dataset = CustomImageDataset(mapped_labels, img_dir)\n",
    "dataset2 = CustomImageDataset(mapped_labels2, img_dir2)\n",
    "dataset = torch.utils.data.ConcatDataset([dataset, dataset2])\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "train_dataset = TrDataset(train_dataset, transform)\n",
    "val_dataset = TrDataset(val_dataset, transform2)\n",
    "\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=10, persistent_workers = True, prefetch_factor=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Load the pre-trained ResNet-18 model\n",
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "# # Modify the final fully connected layer to match the number of classes\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 1)  # Two classes: 'aligned' and 'not_aligned'\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Load the pre-trained ResNet-18 model\n",
    "# model = models.efficientnet_v2_m('DEFAULT')\n",
    "\n",
    "# # Modify the final fully connected layer to match the number of classes\n",
    "# num_ftrs = model.classifier[-1].out_features\n",
    "# m2 = nn.Linear(num_ftrs, 1)  # Two classes: 'aligned' and 'not_aligned'\n",
    "\n",
    "# model = nn.Sequential(model,nn.ReLU(), m2).to(device)\n",
    "# model = torch.compile(model)\n",
    "\n",
    "torch.set_float32_matmul_precision('high') #'medium' | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "m = BaseModel(model, lr=0.0001)\n",
    "checkpoint = ModelCheckpoint(monitor='val_f_beta', dirpath='checkpoints', filename='model-{epoch:02d}-{val_f_beta:.3f}', save_top_k=1, mode='max')\n",
    "trainer = pl.Trainer(max_epochs=50, accelerator=\"auto\", callbacks=[checkpoint])#, precision=\"16-mixed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /teamspace/studios/this_studio/checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name       | Type              | Params | Mode \n",
      "---------------------------------------------------------\n",
      "0 | model      | ResNet            | 11.2 M | train\n",
      "1 | loss       | BCEWithLogitsLoss | 0      | train\n",
      "2 | f_beta     | BinaryFBetaScore  | 0      | train\n",
      "3 | accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | precision  | BinaryPrecision   | 0      | train\n",
      "5 | recall     | BinaryRecall      | 0      | train\n",
      "6 | last_layer | Linear            | 2      | train\n",
      "---------------------------------------------------------\n",
      "11.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.2 M    Total params\n",
      "44.708    Total estimated model params size (MB)\n",
      "74        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb911be56e8f4a38ad8a02106e30fcce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "495ce01f23544fa982c3a7ac9d5299b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b809f08971844af389715c449588acf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b58ae7b340b34ce7873b8aa97b702ec5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f541e338b7f8431aaa4e1d782fdc4f58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dd58745da6b4e82bf1a6e6def6a9480",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a488b3ac225d45a8987043a2ee2ee016",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "051a6dab1bde4c749c33e9243eb19508",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10e01d8a563a4fd0b7dd1b4f494ba117",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "951cc66a5ab54fc3a637966d6182a0db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d20bd5de19ff4c49b5a8b4ac05c5f2ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fdd39a4d45b43b89317e2259116005a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9382897860b4ae4910f5654be6aa70f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9330db933b7444348be5cfe4f9959093",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f924fc993d54fb19d6c02fc5dadb329",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "066c0c8cafca488aa319f85f14389997",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(m, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = BaseModel.load_from_checkpoint(\"/teamspace/studios/this_studio/checkpoints/best.ckpt\", model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c5570c13adc4cf9a36eb139a11d6fb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       val_accuracy        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9493243098258972     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        val_f_beta         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8290155529975891     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       val_precision       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8421052694320679     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        val_recall         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7804877758026123     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m      val_accuracy       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9493243098258972    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m       val_f_beta        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8290155529975891    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      val_precision      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8421052694320679    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m       val_recall        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7804877758026123    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'val_f_beta': 0.8290155529975891,\n",
       "  'val_accuracy': 0.9493243098258972,\n",
       "  'val_precision': 0.8421052694320679,\n",
       "  'val_recall': 0.7804877758026123}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "\n",
    "# Set device to GPU or CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Define data transformations\n",
    "transform = transforms.Compose([\n",
    "    # transforms.Resize((224, 224)),  # Resize images to 224x224 pixels\n",
    "    cut_in_half,\n",
    "    transforms.ToTensor(),          # Convert PIL Image to tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # Normalize to ImageNet standards\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Specify the directory where your dataset is stored\n",
    "data_dir = '/teamspace/studios/this_studio/data/example_set'  # Replace with your dataset path\n",
    "\n",
    "# Load the dataset using ImageFolder\n",
    "dataset = datasets.ImageFolder(root=data_dir, transform=transform, target_transform=lambda x: torch.tensor([1.]) if x == 0 else torch.tensor([0.]))\n",
    "dataset_loader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "trainer.test(m, dataset_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
